{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification Modeling - r/biology & r/biochemistry Predicting with Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import AdaBoostClassifier,GradientBoostingClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import recall_score, f1_score, precision_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#lemmatizer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# Import stemmer.\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the submissions csv file\n",
    "#get rid of unnamed:0 column wtih index_col\n",
    "submissions = pd.read_csv('datasets/cleaned-submission.csv',index_col=0)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "#remove all words that apply to the target variable -- biology,bio,biochem,biochemistry\n",
    "submissions['selftext'].replace('biology','',regex=True,inplace=True)\n",
    "submissions['selftext'].replace('biochemistry','',regex=True,inplace=True)\n",
    "submissions['selftext'].replace('chemistry','',regex=True,inplace=True)\n",
    "submissions['selftext'].replace('biochem','',regex=True,inplace=True)\n",
    "submissions['selftext'].replace('bio','',regex=True,inplace=True)\n",
    "submissions['selftext'].replace('chem','',regex=True,inplace=True)\n",
    "#drop the null rows\n",
    "submissions.dropna(how='any',axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step over collection of tokens and try to lemmatize each of them\n",
    "#to use in countvectorizer we pass the new class as the tokenizer\n",
    "class LemmaTokenizer:\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "    def __call__(self,doc):\n",
    "        return [self.lemmatizer.lemmatize(t) for t in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#longer list of stop words\n",
    "#taken from the open source work found here: https://gist.github.com/sebleier/554280\n",
    "#txt found here: https://gist.githubusercontent.com/ZohebAbai/513218c3468130eacff6481f424e4e64/raw/b70776f341a148293ff277afa0d0302c8c38f7e2/gist_stopwords.txt\n",
    "\n",
    "stop_word = pd.read_csv('datasets/stopwords.csv',index_col=0)\n",
    "stop_word = list(stop_word['stopwords'])\n",
    "\n",
    "#remove punctuation from the stop words, as it has already been done in cleaning the text\n",
    "stop_word = [word.replace(\"'\",'') for word in stop_word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting modeling with Selftext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up X and y values for modeling\n",
    "X = submissions['selftext']\n",
    "y = np.where(submissions['subreddit']=='Biochemistry',1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train/test split the data\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,stratify=y,random_state = 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Null Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most frequent class is r/biochemistry. The accuracy of the null model is 0.5686.\n"
     ]
    }
   ],
   "source": [
    "#to get the baseline accuracy of the model\n",
    "#based on the most frequent value in the training data\n",
    "#biochemistry = 1, biology = 0\n",
    "\n",
    "biochem_num = y_train.sum()\n",
    "biology_num = len(y_train)-biochem_num\n",
    "\n",
    "if biology_num < biochem_num:\n",
    "    baseline_accur = round(biochem_num/len(y_train),4)\n",
    "    print(f'The most frequent class is r/biochemistry. The accuracy of the null model is {baseline_accur}.')\n",
    "    \n",
    "else:\n",
    "    baseline_accuracy = round((biology_num)/len(y_train),4)\n",
    "    print(f'The most frequent class is r/biology. The accuracy of the null model is {baseline_accuracy}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline/Null Model Explained:\n",
    "\n",
    "The baseline model allows us to find a 'starting point' to compare the performance of future models to. In binary classification, a customary baseline/null model is one that will guess the most frequently occuring class in the testing set. The code above is a way to print out what the baseline accuracy of the null model actually is. With the random state of the train/test split set, this accuracy should not change with future restarts of the kernel. The most frequent class is that of r/biochemistry. Therefore, there is a 56.81% accuracy if you were to guess r/biochemsitry for every observation within the data set. \n",
    "\n",
    "Let's see if we can't improve on this score with some NLP modeling!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoosting with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create adaboost model in pipeline\n",
    "cvect = CountVectorizer(stop_words=stop_word)\n",
    "pipe_adacv = make_pipeline(cvect,StandardScaler(with_mean=False),\n",
    "                           AdaBoostClassifier(base_estimator=MultinomialNB(alpha=250)))\n",
    "\n",
    "#make parameter grid\n",
    "param_adacv = {\n",
    "    'countvectorizer__max_features':[2_500,3_500],\n",
    "    'countvectorizer__ngram_range':[(1,1),(1,2)],\n",
    "    'adaboostclassifier__n_estimators': [1_500,2_500],\n",
    "    'adaboostclassifier__learning_rate':[0.1,0.25]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('countvectorizer',\n",
       "                                        CountVectorizer(stop_words=['0o', '0s',\n",
       "                                                                    '3a', '3b',\n",
       "                                                                    '3d', '6b',\n",
       "                                                                    '6o', 'a',\n",
       "                                                                    'a1', 'a2',\n",
       "                                                                    'a3', 'a4',\n",
       "                                                                    'ab',\n",
       "                                                                    'able',\n",
       "                                                                    'about',\n",
       "                                                                    'above',\n",
       "                                                                    'abst',\n",
       "                                                                    'ac',\n",
       "                                                                    'accordance',\n",
       "                                                                    'according',\n",
       "                                                                    'accordingly',\n",
       "                                                                    'across',\n",
       "                                                                    'act',\n",
       "                                                                    'actually',\n",
       "                                                                    'ad',\n",
       "                                                                    'added',\n",
       "                                                                    'adj', 'ae',\n",
       "                                                                    'af',\n",
       "                                                                    'affected', ...])),\n",
       "                                       ('standardscaler',\n",
       "                                        StandardScaler(with_mean=False)),\n",
       "                                       ('adaboostclassifier',\n",
       "                                        AdaBoostClassifier(base_estimator=MultinomialNB(alpha=250)))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'adaboostclassifier__learning_rate': [0.1, 0.25],\n",
       "                         'adaboostclassifier__n_estimators': [1500, 2500],\n",
       "                         'countvectorizer__max_features': [2500, 3500],\n",
       "                         'countvectorizer__ngram_range': [(1, 1), (1, 2)]})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make gridsearch instance\n",
    "grid_adacv = GridSearchCV(pipe_adacv,param_grid = param_adacv,n_jobs=-1)\n",
    "\n",
    "\n",
    "grid_adacv.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8178493050475494"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the accuracy on the training data\n",
    "grid_adacv.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the AdaBoost model is 0.7279.\n",
      "The recall of the AdaBoost model is 0.8707.\n",
      "The f1 score of the AdaBoost model is 0.7845.\n",
      "The precision score of the AdaBoost model is 0.7138.\n"
     ]
    }
   ],
   "source": [
    "#score on accuracy and recall\n",
    "print(f'The accuracy of the AdaBoost model is {round(grid_adacv.score(X_test,y_test),4)}.')\n",
    "print(f'The recall of the AdaBoost model is {round(recall_score(y_test,grid_adacv.predict(X_test)),4)}.')\n",
    "print(f'The f1 score of the AdaBoost model is {round(f1_score(y_test,grid_adacv.predict(X_test)),4)}.')\n",
    "print(f'The precision score of the AdaBoost model is {round(precision_score(y_test,grid_adacv.predict(X_test)),4)}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_adaboostclassifier__learning_rate</th>\n",
       "      <th>param_adaboostclassifier__n_estimators</th>\n",
       "      <th>param_countvectorizer__max_features</th>\n",
       "      <th>param_countvectorizer__ngram_range</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>12.931333</td>\n",
       "      <td>0.116155</td>\n",
       "      <td>1.547007</td>\n",
       "      <td>0.033536</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2500</td>\n",
       "      <td>3500</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{'adaboostclassifier__learning_rate': 0.1, 'ad...</td>\n",
       "      <td>0.734918</td>\n",
       "      <td>0.730043</td>\n",
       "      <td>0.735366</td>\n",
       "      <td>0.745732</td>\n",
       "      <td>0.737195</td>\n",
       "      <td>0.736651</td>\n",
       "      <td>0.005120</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>14.026517</td>\n",
       "      <td>0.087846</td>\n",
       "      <td>1.635907</td>\n",
       "      <td>0.018600</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2500</td>\n",
       "      <td>3500</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{'adaboostclassifier__learning_rate': 0.1, 'ad...</td>\n",
       "      <td>0.736137</td>\n",
       "      <td>0.727605</td>\n",
       "      <td>0.735366</td>\n",
       "      <td>0.745732</td>\n",
       "      <td>0.734146</td>\n",
       "      <td>0.735797</td>\n",
       "      <td>0.005811</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>9.136817</td>\n",
       "      <td>0.070082</td>\n",
       "      <td>1.038945</td>\n",
       "      <td>0.008278</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1500</td>\n",
       "      <td>3500</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{'adaboostclassifier__learning_rate': 0.25, 'a...</td>\n",
       "      <td>0.734918</td>\n",
       "      <td>0.730652</td>\n",
       "      <td>0.729878</td>\n",
       "      <td>0.742073</td>\n",
       "      <td>0.735976</td>\n",
       "      <td>0.734699</td>\n",
       "      <td>0.004374</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8.005079</td>\n",
       "      <td>0.097236</td>\n",
       "      <td>0.986915</td>\n",
       "      <td>0.005667</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1500</td>\n",
       "      <td>3500</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{'adaboostclassifier__learning_rate': 0.25, 'a...</td>\n",
       "      <td>0.729433</td>\n",
       "      <td>0.728215</td>\n",
       "      <td>0.731707</td>\n",
       "      <td>0.747561</td>\n",
       "      <td>0.729878</td>\n",
       "      <td>0.733359</td>\n",
       "      <td>0.007189</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.876399</td>\n",
       "      <td>0.038841</td>\n",
       "      <td>0.971102</td>\n",
       "      <td>0.005645</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1500</td>\n",
       "      <td>3500</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{'adaboostclassifier__learning_rate': 0.1, 'ad...</td>\n",
       "      <td>0.729433</td>\n",
       "      <td>0.732480</td>\n",
       "      <td>0.731098</td>\n",
       "      <td>0.739634</td>\n",
       "      <td>0.734146</td>\n",
       "      <td>0.733358</td>\n",
       "      <td>0.003501</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "6       12.931333      0.116155         1.547007        0.033536   \n",
       "7       14.026517      0.087846         1.635907        0.018600   \n",
       "11       9.136817      0.070082         1.038945        0.008278   \n",
       "10       8.005079      0.097236         0.986915        0.005667   \n",
       "2        7.876399      0.038841         0.971102        0.005645   \n",
       "\n",
       "   param_adaboostclassifier__learning_rate  \\\n",
       "6                                      0.1   \n",
       "7                                      0.1   \n",
       "11                                    0.25   \n",
       "10                                    0.25   \n",
       "2                                      0.1   \n",
       "\n",
       "   param_adaboostclassifier__n_estimators param_countvectorizer__max_features  \\\n",
       "6                                    2500                                3500   \n",
       "7                                    2500                                3500   \n",
       "11                                   1500                                3500   \n",
       "10                                   1500                                3500   \n",
       "2                                    1500                                3500   \n",
       "\n",
       "   param_countvectorizer__ngram_range  \\\n",
       "6                              (1, 1)   \n",
       "7                              (1, 2)   \n",
       "11                             (1, 2)   \n",
       "10                             (1, 1)   \n",
       "2                              (1, 1)   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "6   {'adaboostclassifier__learning_rate': 0.1, 'ad...           0.734918   \n",
       "7   {'adaboostclassifier__learning_rate': 0.1, 'ad...           0.736137   \n",
       "11  {'adaboostclassifier__learning_rate': 0.25, 'a...           0.734918   \n",
       "10  {'adaboostclassifier__learning_rate': 0.25, 'a...           0.729433   \n",
       "2   {'adaboostclassifier__learning_rate': 0.1, 'ad...           0.729433   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "6            0.730043           0.735366           0.745732   \n",
       "7            0.727605           0.735366           0.745732   \n",
       "11           0.730652           0.729878           0.742073   \n",
       "10           0.728215           0.731707           0.747561   \n",
       "2            0.732480           0.731098           0.739634   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "6            0.737195         0.736651        0.005120                1  \n",
       "7            0.734146         0.735797        0.005811                2  \n",
       "11           0.735976         0.734699        0.004374                3  \n",
       "10           0.729878         0.733359        0.007189                4  \n",
       "2            0.734146         0.733358        0.003501                5  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get results as a dataframe\n",
    "pd.DataFrame(grid_adacv.cv_results_).sort_values(by='rank_test_score').head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Interpretation\n",
    "\n",
    "Performing AdaBoosting with the best CountVectorizer learned so far, Multinomial Naive Bayes. AdaBoost algorithm will take in the weak learner and will iteratively reweight the errors of the subsequent models. This model performs closely to the AdaBoost with TfidfVectorization, but overall it does perform about 1% worse in both accuracy and recall after fitting with this training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoosting with TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create adaboost model in pipeline\n",
    "tfidf_vect = TfidfVectorizer(stop_words=stop_word)\n",
    "pipe_ada_tfidf = make_pipeline(tfidf_vect,StandardScaler(with_mean=False),\n",
    "                               AdaBoostClassifier(base_estimator=MultinomialNB(alpha=630)))\n",
    "\n",
    "#make parameter grid\n",
    "param_ada_tfidf = {\n",
    "    'tfidfvectorizer__max_features':[2_700,2_800,2_900],\n",
    "    'tfidfvectorizer__ngram_range':[(1,1),(1,2)],\n",
    "    'adaboostclassifier__n_estimators': [150,500],\n",
    "    'adaboostclassifier__learning_rate':[0.9,0.95]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('tfidfvectorizer',\n",
       "                                        TfidfVectorizer(stop_words=['0o', '0s',\n",
       "                                                                    '3a', '3b',\n",
       "                                                                    '3d', '6b',\n",
       "                                                                    '6o', 'a',\n",
       "                                                                    'a1', 'a2',\n",
       "                                                                    'a3', 'a4',\n",
       "                                                                    'ab',\n",
       "                                                                    'able',\n",
       "                                                                    'about',\n",
       "                                                                    'above',\n",
       "                                                                    'abst',\n",
       "                                                                    'ac',\n",
       "                                                                    'accordance',\n",
       "                                                                    'according',\n",
       "                                                                    'accordingly',\n",
       "                                                                    'across',\n",
       "                                                                    'act',\n",
       "                                                                    'actually',\n",
       "                                                                    'ad',\n",
       "                                                                    'added',\n",
       "                                                                    'adj', 'ae',\n",
       "                                                                    'af',\n",
       "                                                                    'affected', ...])),\n",
       "                                       ('standardscaler',\n",
       "                                        StandardScaler(with_mean=False)),\n",
       "                                       ('adaboostclassifier',\n",
       "                                        AdaBoostClassifier(base_estimator=MultinomialNB(alpha=630)))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'adaboostclassifier__learning_rate': [0.9, 0.95],\n",
       "                         'adaboostclassifier__n_estimators': [150, 500],\n",
       "                         'tfidfvectorizer__max_features': [2700, 2800, 2900],\n",
       "                         'tfidfvectorizer__ngram_range': [(1, 1), (1, 2)]})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make gridsearch instance\n",
    "grid_ada_tfidf = GridSearchCV(pipe_ada_tfidf,param_grid = param_ada_tfidf,n_jobs=-1)\n",
    "\n",
    "\n",
    "grid_ada_tfidf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8269934162399415"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#score on the training data\n",
    "grid_ada_tfidf.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the AdaBoost model is 0.7436.\n",
      "The recall of the AdaBoost model is 0.872.\n",
      "The f1 score of the AdaBoost model is 0.7946.\n",
      "The precision score of the AdaBoost model is 0.7298.\n"
     ]
    }
   ],
   "source": [
    "#score on accuracy and recall\n",
    "print(f'The accuracy of the AdaBoost model is {round(grid_ada_tfidf.score(X_test,y_test),4)}.')\n",
    "print(f'The recall of the AdaBoost model is {round(recall_score(y_test,grid_ada_tfidf.predict(X_test)),4)}.')\n",
    "print(f'The f1 score of the AdaBoost model is {round(f1_score(y_test,grid_ada_tfidf.predict(X_test)),4)}.')\n",
    "print(f'The precision score of the AdaBoost model is {round(precision_score(y_test,grid_ada_tfidf.predict(X_test)),4)}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_adaboostclassifier__learning_rate</th>\n",
       "      <th>param_adaboostclassifier__n_estimators</th>\n",
       "      <th>param_tfidfvectorizer__max_features</th>\n",
       "      <th>param_tfidfvectorizer__ngram_range</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2.938340</td>\n",
       "      <td>0.043447</td>\n",
       "      <td>0.358331</td>\n",
       "      <td>0.037832</td>\n",
       "      <td>0.95</td>\n",
       "      <td>500</td>\n",
       "      <td>2900</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{'adaboostclassifier__learning_rate': 0.95, 'a...</td>\n",
       "      <td>0.741012</td>\n",
       "      <td>0.750762</td>\n",
       "      <td>0.746951</td>\n",
       "      <td>0.753659</td>\n",
       "      <td>0.743902</td>\n",
       "      <td>0.747257</td>\n",
       "      <td>0.004552</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.900059</td>\n",
       "      <td>0.024293</td>\n",
       "      <td>0.389555</td>\n",
       "      <td>0.009692</td>\n",
       "      <td>0.9</td>\n",
       "      <td>500</td>\n",
       "      <td>2900</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{'adaboostclassifier__learning_rate': 0.9, 'ad...</td>\n",
       "      <td>0.742230</td>\n",
       "      <td>0.750762</td>\n",
       "      <td>0.745732</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>0.740854</td>\n",
       "      <td>0.747135</td>\n",
       "      <td>0.005638</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.956627</td>\n",
       "      <td>0.038928</td>\n",
       "      <td>0.442407</td>\n",
       "      <td>0.006728</td>\n",
       "      <td>0.9</td>\n",
       "      <td>500</td>\n",
       "      <td>2800</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{'adaboostclassifier__learning_rate': 0.9, 'ad...</td>\n",
       "      <td>0.742230</td>\n",
       "      <td>0.756246</td>\n",
       "      <td>0.742683</td>\n",
       "      <td>0.749390</td>\n",
       "      <td>0.738415</td>\n",
       "      <td>0.745793</td>\n",
       "      <td>0.006309</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "22       2.938340      0.043447         0.358331        0.037832   \n",
       "10       2.900059      0.024293         0.389555        0.009692   \n",
       "9        3.956627      0.038928         0.442407        0.006728   \n",
       "\n",
       "   param_adaboostclassifier__learning_rate  \\\n",
       "22                                    0.95   \n",
       "10                                     0.9   \n",
       "9                                      0.9   \n",
       "\n",
       "   param_adaboostclassifier__n_estimators param_tfidfvectorizer__max_features  \\\n",
       "22                                    500                                2900   \n",
       "10                                    500                                2900   \n",
       "9                                     500                                2800   \n",
       "\n",
       "   param_tfidfvectorizer__ngram_range  \\\n",
       "22                             (1, 1)   \n",
       "10                             (1, 1)   \n",
       "9                              (1, 2)   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "22  {'adaboostclassifier__learning_rate': 0.95, 'a...           0.741012   \n",
       "10  {'adaboostclassifier__learning_rate': 0.9, 'ad...           0.742230   \n",
       "9   {'adaboostclassifier__learning_rate': 0.9, 'ad...           0.742230   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "22           0.750762           0.746951           0.753659   \n",
       "10           0.750762           0.745732           0.756098   \n",
       "9            0.756246           0.742683           0.749390   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "22           0.743902         0.747257        0.004552                1  \n",
       "10           0.740854         0.747135        0.005638                2  \n",
       "9            0.738415         0.745793        0.006309                3  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#casting the results as a dataframe\n",
    "pd.DataFrame(grid_ada_tfidf.cv_results_).sort_values(by='rank_test_score').head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Interpretation:\n",
    "\n",
    "AdaBoosting the best model discovered for the evaluation metrics accuracy and recall actually boosted those scores by approximately 1% each. The AdaBoost model is the best model learned yet. It has the highest accuracy and recall pairing out of all of the models. This fits in with the two evaluation metrics I wanted to balance for my data science question. Therefore, this is the model I will chose as the final production model for the NLP classification. This model also shows slight overfitting to the training data, which may skew coefficient values and cause a lack of generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GradientBoost with CountVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create adaboost model in pipeline\n",
    "pipe_gb_cv = make_pipeline(cvect,StandardScaler(with_mean=False),GradientBoostingClassifier())\n",
    "\n",
    "#make parameter grid\n",
    "param_gb_cv = {\n",
    "    'countvectorizer__max_features':[3_750,3_500,4_000],\n",
    "    'countvectorizer__ngram_range':[(1,1),(1,2)],\n",
    "    'gradientboostingclassifier__n_estimators': [250,325],\n",
    "    'gradientboostingclassifier__learning_rate':[0.25,0.3],\n",
    "    'gradientboostingclassifier__min_samples_split': [5,6],\n",
    "    'gradientboostingclassifier__max_depth':[6,7,5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('countvectorizer',\n",
       "                                        CountVectorizer(stop_words=['0o', '0s',\n",
       "                                                                    '3a', '3b',\n",
       "                                                                    '3d', '6b',\n",
       "                                                                    '6o', 'a',\n",
       "                                                                    'a1', 'a2',\n",
       "                                                                    'a3', 'a4',\n",
       "                                                                    'ab',\n",
       "                                                                    'able',\n",
       "                                                                    'about',\n",
       "                                                                    'above',\n",
       "                                                                    'abst',\n",
       "                                                                    'ac',\n",
       "                                                                    'accordance',\n",
       "                                                                    'according',\n",
       "                                                                    'accordingly',\n",
       "                                                                    'across',\n",
       "                                                                    'act',\n",
       "                                                                    'actually',\n",
       "                                                                    'ad',\n",
       "                                                                    'added',\n",
       "                                                                    'adj', 'ae',\n",
       "                                                                    'af',\n",
       "                                                                    'affected', ...])),\n",
       "                                       ('standardscaler',\n",
       "                                        StandardScaler(with_mean=False)),\n",
       "                                       ('...\n",
       "                                        GradientBoostingClassifier())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'countvectorizer__max_features': [3750, 3500, 4000],\n",
       "                         'countvectorizer__ngram_range': [(1, 1), (1, 2)],\n",
       "                         'gradientboostingclassifier__learning_rate': [0.25,\n",
       "                                                                       0.3],\n",
       "                         'gradientboostingclassifier__max_depth': [6, 7, 5],\n",
       "                         'gradientboostingclassifier__min_samples_split': [5,\n",
       "                                                                           6],\n",
       "                         'gradientboostingclassifier__n_estimators': [250,\n",
       "                                                                      325]})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#instantiate paramgrid\n",
    "grid_gb_cv = GridSearchCV(pipe_gb_cv,param_gb_cv,n_jobs=-1)\n",
    "\n",
    "#train on the training data\n",
    "grid_gb_cv.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9642770056083882"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#score on the training data\n",
    "grid_gb_cv.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the Gradient Boost model is 0.7516.\n",
      "The recall of the Gradient Boost model is 0.8026.\n",
      "The f1 score of the Gradient Boost model is 0.7861.\n",
      "The precision score of the Gradient Boost model is 0.7704.\n"
     ]
    }
   ],
   "source": [
    "#score on accuracy and recall\n",
    "print(f'The accuracy of the Gradient Boost model is {round(grid_gb_cv.score(X_test,y_test),4)}.')\n",
    "print(f'The recall of the Gradient Boost model is {round(recall_score(y_test,grid_gb_cv.predict(X_test)),4)}.')\n",
    "print(f'The f1 score of the Gradient Boost model is {round(f1_score(y_test,grid_gb_cv.predict(X_test)),4)}.')\n",
    "print(f'The precision score of the Gradient Boost model is {round(precision_score(y_test,grid_gb_cv.predict(X_test)),4)}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'countvectorizer__max_features': 3750,\n",
       " 'countvectorizer__ngram_range': (1, 2),\n",
       " 'gradientboostingclassifier__learning_rate': 0.3,\n",
       " 'gradientboostingclassifier__max_depth': 7,\n",
       " 'gradientboostingclassifier__min_samples_split': 5,\n",
       " 'gradientboostingclassifier__n_estimators': 325}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the best paramets of the gradient boost\n",
    "grid_gb_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_countvectorizer__max_features</th>\n",
       "      <th>param_countvectorizer__ngram_range</th>\n",
       "      <th>param_gradientboostingclassifier__learning_rate</th>\n",
       "      <th>param_gradientboostingclassifier__max_depth</th>\n",
       "      <th>param_gradientboostingclassifier__min_samples_split</th>\n",
       "      <th>param_gradientboostingclassifier__n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>11.467868</td>\n",
       "      <td>0.051109</td>\n",
       "      <td>0.168353</td>\n",
       "      <td>0.001471</td>\n",
       "      <td>3750</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.3</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>325</td>\n",
       "      <td>{'countvectorizer__max_features': 3750, 'count...</td>\n",
       "      <td>0.742840</td>\n",
       "      <td>0.744059</td>\n",
       "      <td>0.742073</td>\n",
       "      <td>0.745732</td>\n",
       "      <td>0.748780</td>\n",
       "      <td>0.744697</td>\n",
       "      <td>0.002387</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>10.165480</td>\n",
       "      <td>0.032112</td>\n",
       "      <td>0.120109</td>\n",
       "      <td>0.003798</td>\n",
       "      <td>3500</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>0.25</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>325</td>\n",
       "      <td>{'countvectorizer__max_features': 3500, 'count...</td>\n",
       "      <td>0.738574</td>\n",
       "      <td>0.737355</td>\n",
       "      <td>0.745732</td>\n",
       "      <td>0.747561</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.743844</td>\n",
       "      <td>0.005003</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>9.962712</td>\n",
       "      <td>0.071769</td>\n",
       "      <td>0.164252</td>\n",
       "      <td>0.001687</td>\n",
       "      <td>3750</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.25</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>325</td>\n",
       "      <td>{'countvectorizer__max_features': 3750, 'count...</td>\n",
       "      <td>0.738574</td>\n",
       "      <td>0.746496</td>\n",
       "      <td>0.740854</td>\n",
       "      <td>0.751220</td>\n",
       "      <td>0.737805</td>\n",
       "      <td>0.742990</td>\n",
       "      <td>0.005118</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10.309314</td>\n",
       "      <td>0.046702</td>\n",
       "      <td>0.119610</td>\n",
       "      <td>0.002610</td>\n",
       "      <td>3750</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>0.3</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>325</td>\n",
       "      <td>{'countvectorizer__max_features': 3750, 'count...</td>\n",
       "      <td>0.731261</td>\n",
       "      <td>0.744668</td>\n",
       "      <td>0.741463</td>\n",
       "      <td>0.753049</td>\n",
       "      <td>0.743902</td>\n",
       "      <td>0.742869</td>\n",
       "      <td>0.006995</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>9.725709</td>\n",
       "      <td>0.059658</td>\n",
       "      <td>0.163148</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>3500</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.25</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>325</td>\n",
       "      <td>{'countvectorizer__max_features': 3500, 'count...</td>\n",
       "      <td>0.741012</td>\n",
       "      <td>0.741621</td>\n",
       "      <td>0.743293</td>\n",
       "      <td>0.742073</td>\n",
       "      <td>0.744512</td>\n",
       "      <td>0.742502</td>\n",
       "      <td>0.001253</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "41      11.467868      0.051109         0.168353        0.001471   \n",
       "55      10.165480      0.032112         0.120109        0.003798   \n",
       "27       9.962712      0.071769         0.164252        0.001687   \n",
       "19      10.309314      0.046702         0.119610        0.002610   \n",
       "75       9.725709      0.059658         0.163148        0.002100   \n",
       "\n",
       "   param_countvectorizer__max_features param_countvectorizer__ngram_range  \\\n",
       "41                                3750                             (1, 2)   \n",
       "55                                3500                             (1, 1)   \n",
       "27                                3750                             (1, 2)   \n",
       "19                                3750                             (1, 1)   \n",
       "75                                3500                             (1, 2)   \n",
       "\n",
       "   param_gradientboostingclassifier__learning_rate  \\\n",
       "41                                             0.3   \n",
       "55                                            0.25   \n",
       "27                                            0.25   \n",
       "19                                             0.3   \n",
       "75                                            0.25   \n",
       "\n",
       "   param_gradientboostingclassifier__max_depth  \\\n",
       "41                                           7   \n",
       "55                                           7   \n",
       "27                                           6   \n",
       "19                                           7   \n",
       "75                                           6   \n",
       "\n",
       "   param_gradientboostingclassifier__min_samples_split  \\\n",
       "41                                                  5    \n",
       "55                                                  6    \n",
       "27                                                  6    \n",
       "19                                                  6    \n",
       "75                                                  6    \n",
       "\n",
       "   param_gradientboostingclassifier__n_estimators  \\\n",
       "41                                            325   \n",
       "55                                            325   \n",
       "27                                            325   \n",
       "19                                            325   \n",
       "75                                            325   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "41  {'countvectorizer__max_features': 3750, 'count...           0.742840   \n",
       "55  {'countvectorizer__max_features': 3500, 'count...           0.738574   \n",
       "27  {'countvectorizer__max_features': 3750, 'count...           0.738574   \n",
       "19  {'countvectorizer__max_features': 3750, 'count...           0.731261   \n",
       "75  {'countvectorizer__max_features': 3500, 'count...           0.741012   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "41           0.744059           0.742073           0.745732   \n",
       "55           0.737355           0.745732           0.747561   \n",
       "27           0.746496           0.740854           0.751220   \n",
       "19           0.744668           0.741463           0.753049   \n",
       "75           0.741621           0.743293           0.742073   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "41           0.748780         0.744697        0.002387                1  \n",
       "55           0.750000         0.743844        0.005003                2  \n",
       "27           0.737805         0.742990        0.005118                3  \n",
       "19           0.743902         0.742869        0.006995                4  \n",
       "75           0.744512         0.742502        0.001253                5  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get results of the grid cross validations as a dataframe\n",
    "pd.DataFrame(grid_gb_cv.cv_results_).sort_values(by='rank_test_score').head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Interpretation\n",
    "\n",
    "In the GradientBoost model with CountVectorization there was a decrease in score from the AdaBoost model with CountVectorization of about 10% of the recall score. This means that the Gradient Boost model wasn't predicting true positive as well and was classifying more r/biochemistry posts r/biology posts. This Gradient Boost model also shows high overfitting, therefore it is not trustworthy in terms of coefficient values and a lack of generazability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boost with TfidfVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create adaboost model in pipeline\n",
    "tfidf_vect = TfidfVectorizer(stop_words=stop_word)\n",
    "pipe_gb_tfidf = make_pipeline(tfidf_vect,StandardScaler(with_mean=False),\n",
    "                               GradientBoostingClassifier())\n",
    "\n",
    "#make parameter grid\n",
    "param_gb_tfidf = {\n",
    "    'tfidfvectorizer__max_features':[500,1_000,2_000],\n",
    "    'tfidfvectorizer__ngram_range':[(1,1),(1,2)],\n",
    "    'gradientboostingclassifier__n_estimators': [150,300],\n",
    "    'gradientboostingclassifier__learning_rate':[0.25,0.5,1],\n",
    "    'gradientboostingclassifier__min_samples_split': [3,5],\n",
    "    'gradientboostingclassifier__max_depth':[5,7]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('tfidfvectorizer',\n",
       "                                        TfidfVectorizer(stop_words=['0o', '0s',\n",
       "                                                                    '3a', '3b',\n",
       "                                                                    '3d', '6b',\n",
       "                                                                    '6o', 'a',\n",
       "                                                                    'a1', 'a2',\n",
       "                                                                    'a3', 'a4',\n",
       "                                                                    'ab',\n",
       "                                                                    'able',\n",
       "                                                                    'about',\n",
       "                                                                    'above',\n",
       "                                                                    'abst',\n",
       "                                                                    'ac',\n",
       "                                                                    'accordance',\n",
       "                                                                    'according',\n",
       "                                                                    'accordingly',\n",
       "                                                                    'across',\n",
       "                                                                    'act',\n",
       "                                                                    'actually',\n",
       "                                                                    'ad',\n",
       "                                                                    'added',\n",
       "                                                                    'adj', 'ae',\n",
       "                                                                    'af',\n",
       "                                                                    'affected', ...])),\n",
       "                                       ('standardscaler',\n",
       "                                        StandardScaler(with_mean=False)),\n",
       "                                       ('...\n",
       "                                        GradientBoostingClassifier())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'gradientboostingclassifier__learning_rate': [0.25,\n",
       "                                                                       0.5, 1],\n",
       "                         'gradientboostingclassifier__max_depth': [5, 7],\n",
       "                         'gradientboostingclassifier__min_samples_split': [3,\n",
       "                                                                           5],\n",
       "                         'gradientboostingclassifier__n_estimators': [150, 300],\n",
       "                         'tfidfvectorizer__max_features': [500, 1000, 2000],\n",
       "                         'tfidfvectorizer__ngram_range': [(1, 1), (1, 2)]})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#instantiate paramgrid\n",
    "grid_gb_tfidf = GridSearchCV(pipe_gb_tfidf,param_gb_tfidf,n_jobs=-1)\n",
    "\n",
    "#train on the training data\n",
    "grid_gb_tfidf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96525237746891"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#score on the training data\n",
    "grid_gb_tfidf.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the Gradient Boost model is 0.7352.\n",
      "The recall of the Gradient Boost model is 0.7987.\n",
      "The f1 score of the Gradient Boost model is 0.7743.\n",
      "The precision score of the Gradient Boost model is 0.7514.\n"
     ]
    }
   ],
   "source": [
    "#score on accuracy and recall\n",
    "print(f'The accuracy of the Gradient Boost model is {round(grid_gb_tfidf.score(X_test,y_test),4)}.')\n",
    "print(f'The recall of the Gradient Boost model is {round(recall_score(y_test,grid_gb_tfidf.predict(X_test)),4)}.')\n",
    "print(f'The f1 score of the Gradient Boost model is {round(f1_score(y_test,grid_gb_tfidf.predict(X_test)),4)}.')\n",
    "print(f'The precision score of the Gradient Boost model is {round(precision_score(y_test,grid_gb_tfidf.predict(X_test)),4)}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gradientboostingclassifier__learning_rate': 0.25,\n",
       " 'gradientboostingclassifier__max_depth': 5,\n",
       " 'gradientboostingclassifier__min_samples_split': 5,\n",
       " 'gradientboostingclassifier__n_estimators': 300,\n",
       " 'tfidfvectorizer__max_features': 2000,\n",
       " 'tfidfvectorizer__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#best parameters for the gridsearch model\n",
    "grid_gb_tfidf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_gradientboostingclassifier__learning_rate</th>\n",
       "      <th>param_gradientboostingclassifier__max_depth</th>\n",
       "      <th>param_gradientboostingclassifier__min_samples_split</th>\n",
       "      <th>param_gradientboostingclassifier__n_estimators</th>\n",
       "      <th>param_tfidfvectorizer__max_features</th>\n",
       "      <th>param_tfidfvectorizer__ngram_range</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>14.405022</td>\n",
       "      <td>0.048160</td>\n",
       "      <td>0.153340</td>\n",
       "      <td>0.004449</td>\n",
       "      <td>0.25</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>300</td>\n",
       "      <td>2000</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.719074</td>\n",
       "      <td>0.727605</td>\n",
       "      <td>0.736585</td>\n",
       "      <td>0.734756</td>\n",
       "      <td>0.734756</td>\n",
       "      <td>0.730555</td>\n",
       "      <td>0.006514</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>13.619877</td>\n",
       "      <td>0.074639</td>\n",
       "      <td>0.107298</td>\n",
       "      <td>0.001835</td>\n",
       "      <td>0.25</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>300</td>\n",
       "      <td>2000</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.714199</td>\n",
       "      <td>0.725168</td>\n",
       "      <td>0.739634</td>\n",
       "      <td>0.731707</td>\n",
       "      <td>0.737805</td>\n",
       "      <td>0.729703</td>\n",
       "      <td>0.009265</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>18.384431</td>\n",
       "      <td>0.047740</td>\n",
       "      <td>0.111301</td>\n",
       "      <td>0.001722</td>\n",
       "      <td>0.25</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>300</td>\n",
       "      <td>2000</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.729433</td>\n",
       "      <td>0.725777</td>\n",
       "      <td>0.721951</td>\n",
       "      <td>0.730488</td>\n",
       "      <td>0.733537</td>\n",
       "      <td>0.728237</td>\n",
       "      <td>0.004004</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>19.534681</td>\n",
       "      <td>0.066273</td>\n",
       "      <td>0.158344</td>\n",
       "      <td>0.005918</td>\n",
       "      <td>0.25</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>300</td>\n",
       "      <td>2000</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.716636</td>\n",
       "      <td>0.731871</td>\n",
       "      <td>0.732927</td>\n",
       "      <td>0.728049</td>\n",
       "      <td>0.731098</td>\n",
       "      <td>0.728116</td>\n",
       "      <td>0.005965</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>18.664363</td>\n",
       "      <td>0.038445</td>\n",
       "      <td>0.112302</td>\n",
       "      <td>0.002928</td>\n",
       "      <td>0.25</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>300</td>\n",
       "      <td>2000</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.716636</td>\n",
       "      <td>0.722730</td>\n",
       "      <td>0.732927</td>\n",
       "      <td>0.728049</td>\n",
       "      <td>0.736585</td>\n",
       "      <td>0.727385</td>\n",
       "      <td>0.007113</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "23      14.405022      0.048160         0.153340        0.004449   \n",
       "10      13.619877      0.074639         0.107298        0.001835   \n",
       "46      18.384431      0.047740         0.111301        0.001722   \n",
       "47      19.534681      0.066273         0.158344        0.005918   \n",
       "34      18.664363      0.038445         0.112302        0.002928   \n",
       "\n",
       "   param_gradientboostingclassifier__learning_rate  \\\n",
       "23                                            0.25   \n",
       "10                                            0.25   \n",
       "46                                            0.25   \n",
       "47                                            0.25   \n",
       "34                                            0.25   \n",
       "\n",
       "   param_gradientboostingclassifier__max_depth  \\\n",
       "23                                           5   \n",
       "10                                           5   \n",
       "46                                           7   \n",
       "47                                           7   \n",
       "34                                           7   \n",
       "\n",
       "   param_gradientboostingclassifier__min_samples_split  \\\n",
       "23                                                  5    \n",
       "10                                                  3    \n",
       "46                                                  5    \n",
       "47                                                  5    \n",
       "34                                                  3    \n",
       "\n",
       "   param_gradientboostingclassifier__n_estimators  \\\n",
       "23                                            300   \n",
       "10                                            300   \n",
       "46                                            300   \n",
       "47                                            300   \n",
       "34                                            300   \n",
       "\n",
       "   param_tfidfvectorizer__max_features param_tfidfvectorizer__ngram_range  \\\n",
       "23                                2000                             (1, 2)   \n",
       "10                                2000                             (1, 1)   \n",
       "46                                2000                             (1, 1)   \n",
       "47                                2000                             (1, 2)   \n",
       "34                                2000                             (1, 1)   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "23  {'gradientboostingclassifier__learning_rate': ...           0.719074   \n",
       "10  {'gradientboostingclassifier__learning_rate': ...           0.714199   \n",
       "46  {'gradientboostingclassifier__learning_rate': ...           0.729433   \n",
       "47  {'gradientboostingclassifier__learning_rate': ...           0.716636   \n",
       "34  {'gradientboostingclassifier__learning_rate': ...           0.716636   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "23           0.727605           0.736585           0.734756   \n",
       "10           0.725168           0.739634           0.731707   \n",
       "46           0.725777           0.721951           0.730488   \n",
       "47           0.731871           0.732927           0.728049   \n",
       "34           0.722730           0.732927           0.728049   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "23           0.734756         0.730555        0.006514                1  \n",
       "10           0.737805         0.729703        0.009265                2  \n",
       "46           0.733537         0.728237        0.004004                3  \n",
       "47           0.731098         0.728116        0.005965                4  \n",
       "34           0.736585         0.727385        0.007113                5  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#casting the grid results as a dataframe\n",
    "pd.DataFrame(grid_gb_tfidf.cv_results_).sort_values(by='rank_test_score').head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Interpretation:\n",
    "\n",
    "Similar to the Gradient Boosting model with CountVectorization, the Gradient Boosting model with TfidfVectorization shows high overfitting to the training data. There is also the 10% drop in recall score from the AdaBoost model with TfidfVectorization present in this model. As we are trying to optimize recall score, this model is not what we want going forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost with Combined Text, CountVectorization, Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-Modeling Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the combined text csv\n",
    "combined = pd.read_csv('datasets/combined_title_self.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 22014 entries, 0 to 22013\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   subreddit  22014 non-null  object\n",
      " 1   text       21943 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 516.0+ KB\n"
     ]
    }
   ],
   "source": [
    "#shows that there are null values in the datafile\n",
    "combined.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the null values from the dataframe\n",
    "combined.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove all words that apply to the target variable -- biology,bio,biochem,biochemistry\n",
    "combined['text'].replace('biology','',regex=True,inplace=True)\n",
    "combined['text'].replace('biochemistry','',regex=True,inplace=True)\n",
    "combined['text'].replace('chemistry','',regex=True,inplace=True)\n",
    "combined['text'].replace('biochem','',regex=True,inplace=True)\n",
    "combined['text'].replace('bio','',regex=True,inplace=True)\n",
    "combined['text'].replace('chem','',regex=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up initial X and y values\n",
    "#1 corresponds to biochemistry, 0 corresponds to biology\n",
    "#need to make the str unicode with astype, emoji characters in the strings\n",
    "#help with the unicode error found below:\n",
    "#https://stackoverflow.com/questions/39303912/tfidfvectorizer-in-scikit-learn-valueerror-np-nan-is-an-invalid-document\n",
    "X_combo = combined['text']\n",
    "y = np.where(combined['subreddit']=='Biochemistry',1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train/test split of the data\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_combo,y,stratify=y,random_state=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most frequent class is r/biochemistry. The accuracy of the null model is 0.5673.\n"
     ]
    }
   ],
   "source": [
    "#to get the baseline accuracy of the model\n",
    "#based on the most frequent value in the training data\n",
    "#biochemistry = 1, biology = 0\n",
    "\n",
    "biochem_num = y_train.sum()\n",
    "biology_num = len(y_train)-biochem_num\n",
    "\n",
    "if biology_num < biochem_num:\n",
    "    baseline_accur = round(biochem_num/len(y_train),4)\n",
    "    print(f'The most frequent class is r/biochemistry. The accuracy of the null model is {baseline_accur}.')\n",
    "    \n",
    "else:\n",
    "    baseline_accuracy = round((biology_num)/len(y_train),4)\n",
    "    print(f'The most frequent class is r/biology. The accuracy of the null model is {baseline_accuracy}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Null Model Explained:\n",
    "\n",
    "The baseline model allows us to find a 'starting point' to compare the performance of future models to. In binary classification, a customary baseline/null model is one that will guess the most frequently occuring class in the testing set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model One. Combined Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create adaboost model in pipeline\n",
    "cvect = CountVectorizer(stop_words=stop_word)\n",
    "pipe_adacv_title = make_pipeline(cvect,StandardScaler(with_mean=False),\n",
    "                           AdaBoostClassifier(base_estimator=MultinomialNB(alpha=500)))\n",
    "\n",
    "#make parameter grid\n",
    "param_adacv_titl = {\n",
    "    'countvectorizer__max_features':[6_000,6_250,6_500],\n",
    "    'countvectorizer__ngram_range':[(1,1),(1,2)],\n",
    "    'adaboostclassifier__n_estimators': [350,650,850],\n",
    "    'adaboostclassifier__learning_rate':[.75,1,1.25,1.5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('countvectorizer',\n",
       "                                        CountVectorizer(stop_words=['0o', '0s',\n",
       "                                                                    '3a', '3b',\n",
       "                                                                    '3d', '6b',\n",
       "                                                                    '6o', 'a',\n",
       "                                                                    'a1', 'a2',\n",
       "                                                                    'a3', 'a4',\n",
       "                                                                    'ab',\n",
       "                                                                    'able',\n",
       "                                                                    'about',\n",
       "                                                                    'above',\n",
       "                                                                    'abst',\n",
       "                                                                    'ac',\n",
       "                                                                    'accordance',\n",
       "                                                                    'according',\n",
       "                                                                    'accordingly',\n",
       "                                                                    'across',\n",
       "                                                                    'act',\n",
       "                                                                    'actually',\n",
       "                                                                    'ad',\n",
       "                                                                    'added',\n",
       "                                                                    'adj', 'ae',\n",
       "                                                                    'af',\n",
       "                                                                    'affected', ...])),\n",
       "                                       ('standardscaler',\n",
       "                                        StandardScaler(with_mean=False)),\n",
       "                                       ('adaboostclassifier',\n",
       "                                        AdaBoostClassifier(base_estimator=MultinomialNB(alpha=500)))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'adaboostclassifier__learning_rate': [0.75, 1, 1.25,\n",
       "                                                               1.5],\n",
       "                         'adaboostclassifier__n_estimators': [350, 650, 850],\n",
       "                         'countvectorizer__max_features': [6000, 6250, 6500],\n",
       "                         'countvectorizer__ngram_range': [(1, 1), (1, 2)]})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make gridsearch instance\n",
    "grid_ada_title = GridSearchCV(pipe_adacv_title,param_grid = param_adacv_titl,n_jobs=-1)\n",
    "\n",
    "\n",
    "grid_ada_title.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'adaboostclassifier__learning_rate': 1.25,\n",
       " 'adaboostclassifier__n_estimators': 850,\n",
       " 'countvectorizer__max_features': 6500,\n",
       " 'countvectorizer__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#best parameters\n",
    "grid_ada_title.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8263352980494623"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scoring on the training data\n",
    "grid_ada_title.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the Naive Bayes model with combined text is 0.7366.\n",
      "The recall of the Naive Bayes model with combined text is 0.8705.\n",
      "The f1 score of the Naive Bayes model with combined text is 0.7895.\n",
      "The precision score of the Naive Bayes model with combined text is 0.7222.\n"
     ]
    }
   ],
   "source": [
    "#score on different classification metrics\n",
    "print(f'The accuracy of the Naive Bayes model with combined text is {round(grid_ada_title.score(X_test,y_test),4)}.')\n",
    "print(f'The recall of the Naive Bayes model with combined text is {round(recall_score(y_test,grid_ada_title.predict(X_test)),4)}.')\n",
    "print(f'The f1 score of the Naive Bayes model with combined text is {round(f1_score(y_test,grid_ada_title.predict(X_test)),4)}.')\n",
    "print(f'The precision score of the Naive Bayes model with combined text is {round(precision_score(y_test,grid_ada_title.predict(X_test)),4)}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_adaboostclassifier__learning_rate</th>\n",
       "      <th>param_adaboostclassifier__n_estimators</th>\n",
       "      <th>param_countvectorizer__max_features</th>\n",
       "      <th>param_countvectorizer__ngram_range</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>7.552899</td>\n",
       "      <td>0.045700</td>\n",
       "      <td>0.931866</td>\n",
       "      <td>0.031609</td>\n",
       "      <td>1.25</td>\n",
       "      <td>850</td>\n",
       "      <td>6500</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{'adaboostclassifier__learning_rate': 1.25, 'a...</td>\n",
       "      <td>0.722965</td>\n",
       "      <td>0.735419</td>\n",
       "      <td>0.743239</td>\n",
       "      <td>0.738377</td>\n",
       "      <td>0.723184</td>\n",
       "      <td>0.732637</td>\n",
       "      <td>0.008198</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>7.443069</td>\n",
       "      <td>0.127319</td>\n",
       "      <td>0.799943</td>\n",
       "      <td>0.092289</td>\n",
       "      <td>1.5</td>\n",
       "      <td>850</td>\n",
       "      <td>6500</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{'adaboostclassifier__learning_rate': 1.5, 'ad...</td>\n",
       "      <td>0.722661</td>\n",
       "      <td>0.735419</td>\n",
       "      <td>0.739897</td>\n",
       "      <td>0.741416</td>\n",
       "      <td>0.722881</td>\n",
       "      <td>0.732455</td>\n",
       "      <td>0.008149</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>7.580800</td>\n",
       "      <td>0.076546</td>\n",
       "      <td>0.896024</td>\n",
       "      <td>0.019712</td>\n",
       "      <td>1</td>\n",
       "      <td>850</td>\n",
       "      <td>6500</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{'adaboostclassifier__learning_rate': 1, 'adab...</td>\n",
       "      <td>0.723876</td>\n",
       "      <td>0.736027</td>\n",
       "      <td>0.742328</td>\n",
       "      <td>0.734123</td>\n",
       "      <td>0.723488</td>\n",
       "      <td>0.731968</td>\n",
       "      <td>0.007291</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "52       7.552899      0.045700         0.931866        0.031609   \n",
       "70       7.443069      0.127319         0.799943        0.092289   \n",
       "34       7.580800      0.076546         0.896024        0.019712   \n",
       "\n",
       "   param_adaboostclassifier__learning_rate  \\\n",
       "52                                    1.25   \n",
       "70                                     1.5   \n",
       "34                                       1   \n",
       "\n",
       "   param_adaboostclassifier__n_estimators param_countvectorizer__max_features  \\\n",
       "52                                    850                                6500   \n",
       "70                                    850                                6500   \n",
       "34                                    850                                6500   \n",
       "\n",
       "   param_countvectorizer__ngram_range  \\\n",
       "52                             (1, 1)   \n",
       "70                             (1, 1)   \n",
       "34                             (1, 1)   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "52  {'adaboostclassifier__learning_rate': 1.25, 'a...           0.722965   \n",
       "70  {'adaboostclassifier__learning_rate': 1.5, 'ad...           0.722661   \n",
       "34  {'adaboostclassifier__learning_rate': 1, 'adab...           0.723876   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "52           0.735419           0.743239           0.738377   \n",
       "70           0.735419           0.739897           0.741416   \n",
       "34           0.736027           0.742328           0.734123   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "52           0.723184         0.732637        0.008198                1  \n",
       "70           0.722881         0.732455        0.008149                2  \n",
       "34           0.723488         0.731968        0.007291                3  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataframe of grid results\n",
    "pd.DataFrame(grid_ada_title.cv_results_).sort_values(by='rank_test_score').head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Interpretation\n",
    "\n",
    "The AdaBoost model for combined text with CountVectorizatizer performed similarly to the MultinomialNB model with combined text, with an accuracy of 74.3% and recall of 86.6%. The major difference is that the AdaBoost shows more overfitting (higher variance), than the MultinomialNB model. Due to this fact, I have higher confidence in the MultinomialNB model and its coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting Models Takeaway\n",
    "\n",
    "The best model obtained from the project came from this notebook - AdaBoosting with TfidfVectorization. This model obtains about 76% accuracy and 87% recall score, the highest scoring pair of the evaluation metrics I set out to optimize. AdaBoost with TfidfVectorization is hereby chosen as the final production model! Go Boosting!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
